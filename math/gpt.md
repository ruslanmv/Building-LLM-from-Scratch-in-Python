# Building a Basic GPT Model from Scratch in Python: A Mathematical Breakdown with a Practical Example

## Introduction

In this blog post, we will dive into the mathematical foundations of building a basic GPT (Generative Pretrained Transformer) model from scratch using Python and PyTorch. This architecture follows the principles used in OpenAI's GPT models, which leverage multi-head self-attention and transformer blocks. To solidify our understanding, we will provide concrete examples and explain how they correspond to the mathematical concepts behind the model.

By the end of this post, you will have a solid understanding of how a GPT model processes sequences, the mathematical principles behind the attention mechanism, and how text is generated using a trained GPT model. Additionally, we will clarify how these mathematical principles apply to the practical code example.

---

## Key Components of the GPT Model

### 1. **Multi-Head Self-Attention Mechanism**

The self-attention mechanism is the foundation of the GPT architecture. It enables the model to generate contextualized representations of each token by allowing tokens to "attend" to other tokens in the input sequence.

The mathematical formulation for scaled dot-product attention is:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V
\]

Where:
- \( Q \) (query), \( K \) (key), and \( V \) (value) are matrices representing the tokens.
- \( d_k \) is the dimensionality of the keys and queries.
- The softmax function normalizes the attention scores.

**How this relates to the code:**
- In the Python implementation, the `MultiHeadSelfAttention` class computes the attention scores between tokens. The attention scores are then used to compute a weighted sum of values for each token, reflecting how much attention a token should give to others.

```python
# Compute energy (QK^T / sqrt(d_k))
energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])

# Apply softmax normalization to get attention
attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)

# Compute weighted sum of values
out = torch.einsum("nhql,nlhd->nqhd", [attention, values])
```

In this code, tokens in a sequence attend to one another to build contextualized representations. This is directly aligned with the mathematical formulation of attention.

### 2. **Positional Encoding**

Transformers are inherently order-agnostic, meaning they do not understand the sequence of tokens unless we explicitly provide positional information. This is achieved through **positional encoding**, which assigns a unique position to each token in the sequence.

The mathematical formulation for positional encoding is:

\[
PE(pos, 2i) = \sin\left( \frac{pos}{10000^{2i/d_{\text{model}}}} \right)
\]
\[
PE(pos, 2i+1) = \cos\left( \frac{pos}{10000^{2i/d_{\text{model}}}} \right)
\]

This ensures that each token has a unique position in the sequence.

**How this relates to the code:**
- In the `PositionalEncoding` class, the positional encodings are added to the word embeddings to provide the model with information about the position of each token.

```python
encoding[:, 0::2] = torch.sin(pos / (10000 ** (two_i / self.embed_size)))
encoding[:, 1::2] = torch.cos(pos / (10000 ** (two_i / self.embed_size)))
```

These encodings are added to the token embeddings to ensure that the model understands the position of tokens in the sequence.

### 3. **Transformer Block**

The transformer block is the core computational unit in a GPT model. It combines multi-head self-attention with a feed-forward neural network (FFNN). The FFNN is responsible for refining the token representations generated by the attention mechanism.

The FFNN applied to each token is defined as:

\[
\text{FFNN}(x) = \text{ReLU}(W_1 x + b_1) \times W_2 + b_2
\]

Where:
- \( W_1 \) and \( W_2 \) are weight matrices.
- \( b_1 \) and \( b_2 \) are bias terms.
- ReLU is the activation function used to introduce non-linearity.

**How this relates to the code:**
- The `TransformerBlock` class applies multi-head self-attention followed by a feed-forward network.

```python
forward = self.feed_forward(x)
out = self.norm2(forward + x)  # Apply LayerNorm for stability
```

Layer normalization ensures stability during training by normalizing the outputs from each transformer block.

### 4. **GPT Model Architecture**

A GPT model is a stack of transformer blocks that process input sequences, progressively building richer representations of the tokens. The model generates text by predicting the next token in the sequence based on the previous ones.

The model is mathematically formulated as:

\[
P(y_t | x_1, x_2, ..., x_{t-1}) = \text{softmax}(W \cdot h_t)
\]

Where \( h_t \) is the hidden state of the token at position \( t \), and \( W \) is the projection matrix that maps hidden states back into the vocabulary space.

**How this relates to the code:**
- In the `GPT` class, the forward pass processes the input sequence and generates logits for the next token.

```python
# Get word embeddings and positional encodings
word_embeddings = self.word_embedding(x)
position_encodings = self.position_embedding(word_embeddings)

# Pass through each Transformer block
for layer in self.layers:
    out = layer(out, out, out, mask)

logits = self.fc_out(out)
```

This allows the model to generate a probability distribution over the vocabulary for the next token.

---

## Practical Example: Generating Text with GPT

### 1. **Dataset and Tokenization**

To train the GPT model, we need to process a dataset of text. In this case, we use a small sample dataset:

```python
text = """
The quick brown fox jumps over the lazy dog. 
This is an example of a small dataset for training a GPT model.
We are building a transformer-based architecture.
"""
```

**Mathematical correspondence:**
- The text is tokenized into words, and each word is mapped to a numerical index. Mathematically, this corresponds to converting the text into a sequence of tokens \( X = (x_1, x_2, ..., x_T) \), where each token \( x_i \) represents a word or subword in the vocabulary.

```python
def tokenize(text):
    tokens = re.findall(r'\w+', text.lower())
    return tokens

def build_vocab(text):
    tokens = tokenize(text)
    vocab = Counter(tokens)
    vocab = {word: idx for idx, (word, _) in enumerate(vocab.most_common())}
    return vocab

def encode(text, vocab):
    tokens = tokenize(text)
    return [vocab[token] for token in tokens if token in vocab]
```

The text is broken down into tokens, and a vocabulary is built, mapping each token to a unique index. This is a key step in preparing data for model training.

### 2. **Training the Model**

In the training process, the model is optimized to predict the next token based on the previous tokens in the sequence. During each forward pass, the model computes the probabilities for the next token and compares them with the actual next token in the sequence. The model minimizes the cross-entropy loss:

\[
\mathcal{L} = -\sum_{t=1}^{T} \log P(y_t | x_1, x_2, ..., x_{t-1})
\]

**How this relates to the code:**
- During training, the model learns to minimize the cross-entropy loss by adjusting its parameters.

```python
for batch in data:
    inputs = batch[:, :-1].to(model.device)
    targets = batch[:, 1:].to(model.device)
    output = model(inputs, mask)

    loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))
    loss.backward()
    optimizer.step()
```

This process is repeated over many epochs to ensure the model learns to predict tokens accurately.

### 3. **Example of Text Generation**

Once the model is trained, we can use it to generate text. Given a **prompt**, the model predicts the next token in the sequence, appends it to the input, and repeats the process until the desired length of text is generated.

Let's consider the prompt:

```python
prompt = "The quick brown"
```

**Mathematical correspondence:**
- The input tokens \( X = (x_1, x_2, x_3) \) correspond to the tokens for "The", "quick", and "brown". The model predicts the next token \( y_4 \) by computing:

\[
P(y_4 | x_1, x_2, x_3) = \text{softmax}(W \cdot h_3)
\]

Where \( h_3 \) is the hidden state after processing "The quick brown". The softmax function converts the logits into probabilities, and the most likely next token is selected.

**How this relates to the code:**

```python
for _ in range(max_len):
    output = model(inputs, mask)
    next_token_logits = output[0, -1, :]
    predicted_token = torch.argmax(next

_token_logits).unsqueeze(0).unsqueeze(0)
    inputs = torch.cat([inputs, predicted_token], dim=1)
```

The model continues generating tokens until the specified maximum length is reached.

---

## Conclusion

In this blog post, we've explored the mathematical foundations of building a GPT model from scratch, including multi-head self-attention, positional encoding, and transformer blocks. We also demonstrated how these concepts translate into Python code and applied them to generate text.

By walking through a concrete example of generating text based on a prompt, we've bridged the gap between mathematical theory and practical implementation. With this understanding, you're now ready to explore more advanced concepts in language modeling and GPT architectures!
```